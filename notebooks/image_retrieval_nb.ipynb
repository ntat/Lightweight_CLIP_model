{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import torchvision.datasets as dset\n",
    "\n",
    "from transformers import logging\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from encoder_models import *\n",
    "from utils_clip import *\n",
    "from hooks import *\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Inits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config()\n",
    "\n",
    "checkpoint_path = config['paths']['checkpoint']\n",
    "vit_trans_name = 'google/vit-base-patch16-224'\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "embed_dim = 128 ## make this dynamic\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print (f'Using: {device}')\n",
    "\n",
    "coco_dataset_tst = dset.CocoCaptions(\n",
    "    root=config['data']['test_root'],\n",
    "    annFile=config['data']['test_ann']\n",
    ")\n",
    "\n",
    "test_dataset = CocoCaptionDataset(coco_dataset_tst, mode=\"test\")\n",
    "\n",
    "# instantiate\n",
    "encoder_1 = Transformer_One(vit_trans_name, embed_dim, device=device)\n",
    "encoder_2 = Transformer_Two(bert_model_name, embed_dim, device=device)\n",
    "# load\n",
    "encoder_1, encoder_2, _, _, loss_state_dct, epoch_load, tr_loss, vld_loss = load_model(checkpoint_path, encoder_1, encoder_2, device)\n",
    "logit_sc = loss_state_dct['logit_scale'].exp() ## logit scale\n",
    "# place\n",
    "encoder_1.to(device)\n",
    "encoder_2.to(device)\n",
    "\n",
    "dataloader = DataLoader(test_dataset, batch_size=1000, shuffle=False, drop_last=False, num_workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded model from epoch {epoch_load} with training loss: {tr_loss[-1]:.4f} and validation loss: {vld_loss[-1]:.4f}\")\n",
    "plot_loss(tr_loss, vld_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "# Precompute Image embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = config['paths']['img_embedd']\n",
    "pt_files = [f for f in os.listdir(save_path) if f.endswith('.pt')]\n",
    "\n",
    "if not pt_files:\n",
    "    precompute_img_emb(encoder_1, dataloader, save_path)\n",
    "\n",
    "im_em = load_embeddings(save_path)\n",
    "assert im_em.shape[0] == len(test_dataset), \"Dimension mismatch!\"\n",
    "print (im_em.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# Enter query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt time!\n",
    "query = 'commercial airliners flying in the skies'\n",
    "\n",
    "query_embedding = compute_query_embedding(encoder_2, query, device)\n",
    "top_idxs, sims = find_top_k_matches(im_em, query_embedding, logit_sc, k=5, device=device)\n",
    "\n",
    "encoder_1.model.config.output_attentions=True\n",
    "attention_layers = [encoder_1.model.encoder.layer[x].attention.attention for x in range(len(encoder_1.model.encoder.layer))] \n",
    "image_processor = ViTImageProcessor.from_pretrained(vit_trans_name)\n",
    "\n",
    "attentions = []\n",
    "images = []\n",
    "# probe attention layers\n",
    "with AttentionRecorder(attention_layers) as recorder:\n",
    "    for i in range(len(top_idxs)):\n",
    "        image, _ = coco_dataset_tst[top_idxs[i]]\n",
    "        inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "        inputs = inputs[\"pixel_values\"]\n",
    "        with torch.no_grad():\n",
    "            _ = encoder_1(inputs)\n",
    "        valid_attentions = [attn for attn in recorder.saved_attentions.values() if attn is not None]\n",
    "        attn_map_resized = attention_rollout(valid_attentions, image)\n",
    "        # clear saved attentions for the next image\n",
    "        attentions.append(attn_map_resized)\n",
    "        images.append(image)\n",
    "        recorder.saved_attentions = {i: None for i in recorder.saved_attentions}\n",
    "\n",
    "q_title = f\"Images retrieved for prompt:\\n '{query}'.\"\n",
    "plot_images_and_attentions(images, attentions, title=q_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
